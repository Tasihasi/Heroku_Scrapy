Overview 
----------------------------------------------------------

This is a flask application.

It runs autonomously the daily task and scrapes the Arukereso website.
Link : https://www.arukereso.hu/
The daily task run can be configured on the server where the projects runs buy hitting
the /get_data (whole category scrape) or /start_url_scrape (for the specific urls) api endpoint.

                        +----------------------------------+
                        |          Website Structure       |
                        +----------------------------------+
                                        |
                                        v
        +-------------------+-------------------+-------------------+
        |   Big Category 1  |   Big Category 2  |   Big Category 3  |           Category URL
        +-------------------+-------------------+-------------------+           ============
                    |                   |                   |
                    v                   v                   v
        +----------------+     +----------------+     +----------------+
        |  Product Group |     |  Product Group |     |  Product Group |
        |                |     |                |     |                |          Group URL
        | (Approx. Price)|     | (Approx. Price)|     | (Approx. Price)|        =============
        +----------------+     +----------------+     +----------------+
                    |                   |                   |
                    v                   v                   v
        +-------------------+ +-------------------+ +-------------------+
        | Vendor 1 (Price)  | | Vendor 1 (Price)  | | Vendor 1 (Price)  |
        +-------------------+ +-------------------+ +-------------------+
        | Vendor 2 (Price)  | | Vendor 2 (Price)  | | Vendor 2 (Price)  |
        +-------------------+ +-------------------+ +-------------------+
        | Vendor 3 (Price)  | | Vendor 3 (Price)  | | Vendor 3 (Price)  |
        +-------------------+ +-------------------+ +-------------------+
        


The client can interact through APIs with the server.
The end result is a json file containing : product names, prices, url , category.

The server pushes to google drive automatically the resulting files, 
using the google drive api.

The server is designed that these api endpoint will not be open for public only for 
other servers or front end servers.

Documentation structure.
-> Overview
-> Usage (API description)
-> Technical description

Usage:
--------------------------------------------------
API Endpoints:
    #TODO add appropriate status codes to all api endpoint
    #TODO add api key checker every where!
    # TODO  check that every where shrek key ust be at api keys place 
    # TODO ?? what is the difference between get process data / get business logic data ?
    #TODO ??? difference between aprox scrape and get data

    # TODO  make a class that manages all scrapy management

    # The similar api endpoint responsible for witch url or aprox scrape will start.

    #TODO what is rest api and refactor it to rest api!

    # TODO start_aprox_scrape and url_crawl are the same  !!
    # Find or make the difference !! 

    API  key must be provied in the header as a parameter. 
    Named "shrek_key". (Shrek key is used for internal api calls.)

    Return datatype is a json file.

    API endpoints defined in api_manager/api_route.py:

    -> /ping Method: GET
        Description: Checks if the server is running.

        Request Headers: None
        Request Body: None

        Response:

            -200 OK: Returns JSON with the server status

    -> /check_api_key Method: GET

        Description: Checks if the provided API key is correct.

        Request Headers:

            Authorization (string): shrek_key : "key".
            Request Body: None

        Response:
            -200 OK: Returns JSON if the API key is correct

    -> /get_raw_data    Method: GET
        Description: Retrieves raw data if the API key is valid.

        Request Headers:

            Authorization (string): shrek_key : "key".
            Request Body: None
        
        Response:
            200 OK: Returns the raw data.
            200 OK: If customer status is inactive   #TODO Correcting the api endpoint status code 

            401: Unauthorized: If the API key is incorrect.
            403: Client status is inactive.

    -> /get_data    Method: GET
        Description: Starts the data scraping process asynchronously. Can run multiple scraping processes.

        Request Headers: 
            Authorization (string): shrek_key : "key".   #TODO  no api key needed!
        Request Body: None

        Response:

            200 OK: Returns a message indicating that the spider is running asynchronously.

    -> /get_proxy   #TODO  bad function unnecessary!

    -> /get_final_data  Method: GET
        Description: Retrieves the final raw data from the whole website. Uses data streaming.

        Request Headers:
            Authorization (string): shrek_key : "key".

        Request Body: None

        200 OK: Returns the final data as plain text.
        401 Unauthorized: If the API key is invalid
        404 Not Found: If the file is not found.

    -> /get_processed_data  Method: GET
        Description: Retrieves processed data based on the provided API key.
                    The process is defined in the business logic folder.

        Request Headers: 
            Authorization (string): shrek_key : "key".
        Request Body: None

        Response:
            200 OK: Returns the processed data.
            401 Unauthorized: If no API key is provided or the API key is incorrect.
            500 Internal Server Error: If there is a problem with getting the files from Google Drive.

    -> /customer_data_process Method: GET:
        Description: Runs the data process in the business logic folder.
                    Cannot return partially processed data.

        Request Headers: 
            Authorization (string): shrek_key : "key".

        Request Body: None

        200 OK: If the data processing was successful
        401 Unauthorized: If no API key is provided or the API key is incorrect
        500 Internal Server Error: If data processing failed

    -> /get_business_logic_data     Method: GET:
        Description: Retrieves business logic data by name.

        Request Headers:
            Authorization (string): shrek_key : "key".

        Request Body: None

        Query Parameters:
            file_name (string): The name of the file to retrieve. 
            Default is "customer_min_prices.xml".
        
        Response:
            200 OK: Returns the requested file.
            401 Unauthorized: If no API key is provided or the API key is incorrect.
            404 Not Found: If the file does not exist.

    -> /get_top_5_products  Method: GET:
        Description: Api endpoint for getting the top 5 products. (Used for testing)

        Request Headers: 
            Authorization (string): shrek_key : "key". # TODO  add shrek key checker
        Request Body: None

        Response:

            200 OK: Returns the top 5 products as a JSON file.
            404 Not Found: If the file does not exist.
            500 Internal Server Error: If data processing failed.

    -> /start_aprox_scrape  Method: GET:
        #TODO !!! Implement that category can be specified outside the spider !!!
        It scrapes the category that is specified outside, and returns all the set of group urls. 
        Used in that case when do not want to get the whole category precise details.
        OR when only want to get the urls for that category.

        Request Headers: 
            Authorization (string): shrek_key : "key". # TODO  add a shrek_key checker !! 
        Request Body: None

        Response:
            200 OK: Returns a message indicating that the spider is running.

    -> /get_products_url    Method: GET:
        Description: Returns a set of URLS that is scraped buy the /start_aprox_scrape. 

        Request Headers:
            Authorization (string): shrek_key : "key". !!!! # TODO no api key needed
        Request Body: None

        Response:
            200 OK: Returns the product URLs as a JSON file.
            404 Not Found: If the JSON file is not found
            500 Internal Server Error: If there is an error processing JSON data

    -> /start_url_scrape    Method: POST, GET:
        Description: Starts the URL scrape process with the provided URLs.

        Request Headers: 
            Authorization (string): shrek_key : "key". # TODO  no api key needed !!

        Request Body:
            urls (array): List of URLs to scrape.
            It will be passed as a kwarg to the spider.

        Response:
            200 OK: Returns a message indicating that the spider is running.

    -> /get_url_scrape      Method: GET:
        Description: Retrieves the URL scrape results.

        Request Headers: 
            Authorization (string): shrek_key : "key".   # TODO  no api key needed !!
        Request Body: None

        Response:
            200 OK: Returns the URL scrape results as a JSON file.
            404 Not Found: If the JSON file is not found
            500 Internal Server Error: If there is an error processing JSON data.


    API endpoints defined in api_manager/google_drive/google_drive_api_auth.py:
    !Every google drive authentication data must be provided in the server environment variables.

    #TODO !!there are duplicate api endpoint in the google endpoint!!

    -> /shrek_key_checker METHOD = GET
        Checks if the clients key is correct.

        Possible returns:
            400 - No provided api key
            200 - Correct api key
            403 - Incorrect api key

    -> /list_files METHOD = GET
        Lists the available files in the google drive.

        Request Headers: 
            Authorization (string): shrek_key : "key".   # TODO  no api key needed !!
        Request Body: None

        Possible returns:
            200 :
                {'message': 'No files found'}
                or 
                Json file structured : {name : str, id (google drive file id) : str}
            403 - Incorrect api key
            500 - {'error': 'Authentication failed or an error occurred'}

    -> /get_file/<file_id> METHOD = GET

        Request Headers: 
            Authorization (string): shrek_key : "key".   # TODO  no api key needed !!
        Request Body: None

        Parameter : file_id : str -> Google drive inner file id. Must provide!
        Get the the wanted file id from the /list_files api endpoint.

        When retrieving file even if the file is stored in gzip compression the server returns the uncompressed version.

        Possible returns:
            Directly returns the response from google drive api.
            Uses data streaming. 

            500 : "Error occurred while retrieving the file."

    -> /create_file/<file_name>/<file_mimeType>/<force_update> METHOD = POST
        Uploads a new file in the google drive.

        # TODO  handle cases where the mime type is not defined.

        Request Headers: 
            Authorization (string): shrek_key : "key".   # TODO  no api key needed !!
        Request Body: None

        Parameters:
            - file_name : str - The intended file name - Must provide!
            - file_mimeType : str - The intended file type. Must corresponding to the encoded mime types! - Must provide!
                Possible mime types:
                    - "text"  ->  plain text format.
                    - "ipynb" ->  executable python script.
                    - "csv"   ->  csv file.
                                  Suggestion : use more csv file than txt file for easier data handling.
                    - "gzip"  ->  a compressed gzip file for smaller data amount. 
                                  Suggestion: Use gzip for whole category data storing. 

            - force_update : 0/1 - If wanted to forcefully overwrite the file name even if the file is already exists. - Optional.
                - base value = 0 => no overwrite

        Possible returns:
            - 200 {newly created file id.}
            - 400 {'error': 'File name and MIME type are required.'} or {'error': 'File name already exists.'}
            - 403 {'error': 'Invalid API key'}

    -> /delete_file/<file_id> METHOD = GET
        Deletes an uploaded file from google drive.
        WARNING : There is no confirmation step in deleting the file!

        Request Headers: 
            Authorization (string): shrek_key : "key".   # TODO  no api key needed !!
        Request Body: None

        Parameters:
            -file_name: str - The name of the file to delete. !Must provide!

        Possible returns:
            -403: { "error": "Invalid API key" }
            -200: "File deleted successfully"
            -404: "File not found"
            -500: "An error occurred"

    -> /run_script METHOD = GET
        Runs a Python script from Google Drive.

        Request Headers: 
            Authorization (string): shrek_key : "key".   # TODO  no api key needed !!
        Request Body: None

        Parameters:
            - shrek_key: str - API key in request headers.
            - file_id: str - The ID of the script file to run in request headers.

        Possible returns:
            -200: Script output.
            -400: "Invalid Python code"
            -403: { "error": "Invalid API key" }
            -500: "An error occurred while running the script."

    -> /upload   METHOD  = POST
    -> /download_uploaded/<file_name>   METHOD = GET

    All the api endpoints rely on data_retrieve.py file.
    It manages the run commands for the different spiders.
    It passes the kwargs for the spiders if needed.

There are three big parts of my application.
Components:
-> Main flask server
-> Scraper
    -> Proxy scraper
-> Google Drive communication
    # gzip coding implemented for shorted data amount.

-------------------------------------------------------------
                   The data flow:

            +---------------------------------+
            |       Flask Application         |
            +---------------------------------+
                            |
                            v
            +---------------------------------+
            |  API Endpoint Triggered         |
            +---------------------------------+
                            |
                            v
            +---------------------------------+
            |    Initiate Scrapy Spider       |
            +---------------------------------+
                            |
                            v
            +---------------------------------+
            |      Scrapy Spider Runs         |
            +---------------------------------+
                            |
                            v
            +---------------------------------+
            |  Save Data Locally (Filesystem) |
            +---------------------------------+
                            |
                            v
            +---------------------------------+
            |  Push Data to Google Drive      |
            +---------------------------------+

------------------------------------------------------------
Main Flask server
------------------------------------------------------------
This is the central server that manages all the components.

main.py


------------------------------------------------------------
Scrapy
------------------------------------------------------------
This part is responsible to run the main scraping part.
It contains multiple spiders for different task.

Location: app\heroku_scrapy\heroku_scrapy\spiders

                        Main scraping methods


        +-----------------------------+------------------------------------------------+
        | Whole Website Scraping                                                       |
        +-----------------------------+------------------------------------------------+
        | Description                 | Scrapes the entire website, capturing all      |
        |                             | accessible pages.                              |
        +-----------------------------+------------------------------------------------+
        | Benefits                    | - Comprehensive data collection                |
        |                             | - Ensures no relevant data is missed           |
        +-----------------------------+------------------------------------------------+
        | Drawbacks                   | - Higher resource consumption                  |
        |                             | - Increased processing time                    |
        +-----------------------------+------------------------------------------------+

        +-----------------------------+------------------------------------------------+
        | External Source URLs                                                         |
        +-----------------------------+------------------------------------------------+
        | Description                 | Fetches URLs from an external source and       |
        |                             | scrapes only the provided URLs.                |
        +-----------------------------+------------------------------------------------+
        | Benefits                    | - Targeted data collection                     |
        |                             | - Lower resource consumption                   |
        +-----------------------------+------------------------------------------------+
        | Drawbacks                   | - Potentially misses relevant data not listed  |
        |                             | - Depends on the accuracy of the provided URLs |
        +-----------------------------+------------------------------------------------+

        The different api calls are responsible for witch method is used.


                          Preferred scraping usage
                        +-------------------------+
                        | Scrape the whole website|
                        |   once a week to capture|
                        |   all new products or   |
                        |   URL changes           |
                        +-------------------------+
                                    |
                                    v
                        +-------------------------+
                        |  Scrape the URLs only   |
                        |        daily            |
                        +-------------------------+
                                                    
#TODO  write the exact api endpoint for this scenario.

All scraping methods implement user agent header rotations and proxy rotation.
User agent headers are stored in a txt file.

Proxies are got from a third party website.
Link : https://free-proxy-list.net
 

For every request a different proxy and user agent is used.
If a proxy is still alive it may be used multiple times.

If a proxy is not alive it is deleted from the proxy pool.
Before and during scarping process the proxies are tested and refreshed asynchronously.


How dose url_crawl work?

                +---------------------------------+
                |        URL Scrape API           |
                |  /start_url_scrape is triggered |
                |         Endpoint Triggered      |
                +---------------------------------+
                                |
                                v
                +---------------------------------+
                |    run_url_scrape Function      |
                |    Triggered in data_retrieve.py|
                +---------------------------------+
                                |
                                v
                +---------------------------------+
                |         Command is Run          |
                |  I gets the url list from the   |
                |  api endpoint                   |
                +---------------------------------+
                                |
                                v
                +---------------------------------+
                |     URL Scrape Spider Runs      |
                +---------------------------------+

How dose arukereso_all works?

    
    # TODO multiple check proxy status functions 
    #  -> There is a proxy check outside of the class and there is a inner check proxy 
    # TODO  -> make an proxy manager class that strores and return only a proxy that is needed 
    # TODO      -> Add the proxy checker to that class a function 
    # TODO      -> Add select proxy 

            +----------------------------+
            |       Spider Starts        |
            +----------------------------+
                        |
                        v
            +----------------------------+
            |    User Agent Reading      |
            +----------------------------+
                        |
                        v
            +----------------------------+
            |     Proxy Gathering        |
            +----------------------------+
                        |
                        v
            +----------------------------+
            |      Proxy Testing         |
            +----------------------------+
                        |
                        v
            +----------------------------+
            |  Main Scraping Process     |
            +----------------------------+
                        |
                        v
            +----------------------------+
            | Proxies Available?         |
            |   Yes        |    No       |
            +----------------------------+
            |     |                      |
            |     v                      |
            | Continue                   |
            |     |         |            |
            |     |         v            |
            |     |   +-----------------+|
            |     |   | Proxy Gathering ||
            |     |   +-----------------+|
            |     |         |            |
            |     |         v            |
            |     |   +-----------------+|
            |     |   |  Proxy Testing  ||
            |     |   +-----------------+|
            |     |         |            |
            |     v         v            |
            | +-------------------------+|
            | Main Scraping Process      |
            | +-------------------------+|
            +----------------------------+

    Proxy scraping is made from a third party free api.
    URL : https://api.proxyscrape.com/v2/?request=displayproxies&protocol=http&timeout=10000&country=all&ssl=no&anonymity=elite

    The gathering and testing process runs sumiltaniusly with the main scraping process.

------------------------------------------------------------
Google Drive API
------------------------------------------------------------
    This part is responsible for reading, writing, deleting and creating files to Google Drive.
    Storing data on Google drive is easy and free for a set of data.up to 15 GB.
    ->Thats why i choose to store data in gzip format.

    Google Documentation : https://developers.google.com/drive/api/quickstart/python

    Location : app/api_manager/google_drive

    -> google_drive_api_auth.py
        This part is responsible for authentication for each api call to Google Drive.

        It uses server environment variable.

    -> google_adrive_apiendpoint.py
        Here is defined all the different operation that can be made to Google drive api.



# TODO  delete all chs_get.py files !!


Security:
    # TODO  store the shrek key in hashed format
    # TODO should i implement api key rotation or renewal?

    The api keys are stored on the server side as system variables.

Logging:
    # TODO  should i implement log files?


Performance:
    # TODO create benchmark files that showcase different scenarios.

    Server is using asynchronously and multithreading for boosting performance.
