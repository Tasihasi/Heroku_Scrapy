Overview 
----------------------------------------------------------

This is a flask application.

It runs autonomously the daily task and scrapes the Arukereso website.
Link : https://www.arukereso.hu/

The client can interact through APIs with the server.
The end result is a json file containing : product names, prices, url , category.

The server pushes to google drive automatically the resulting files, 
using the google drive api.

The server is designed that these api endpoint will not be open for public only for 
other servers or front end servers.

Documentation structure.
-> Overview
-> Usage (API description)
-> Technical description

Usage:
--------------------------------------------------
API Endpoints:
    #TODO add appropriate status codes to all api endpoint
    #TODO add api key checker every where!
    # TODO  check that every where shrek key ust be at api keys place 
    # TODO ?? what is the difference between get process data / get business logic data ?
    #TODO ??? difference between aprox scrape and get data

    # The similar api endpoint responsible for witch url or aprox scrape will start.

    #TODO what is rest api and refactor it to rest api!

    API  key must be provied in the header as a parameter. 
    Named "shrek_key". (Shrek key is used for internal api calls.)

    Return datatype is a json file.

    API endpoints defined in api_manager/api_route.py:

    -> /ping Method: GET
        Description: Checks if the server is running.

        Request Headers: None
        Request Body: None

        Response:

            -200 OK: Returns JSON with the server status

    -> /check_api_key Method: GET

        Description: Checks if the provided API key is correct.

        Request Headers:

            Authorization (string): The API key.
            Request Body: None

        Response:
            -200 OK: Returns JSON if the API key is correct

    -> /get_raw_data    Method: GET
        Description: Retrieves raw data if the API key is valid and customer status is active.

        Request Headers:

            Authorization (string): The API key.
            Request Body: None
        
        Response:
            200 OK: Returns the raw data.
            200 OK: If customer status is inactive   #TODO Correcting the api endpoint status code 

            401 Unauthorized: If the API key is incorrect.

    -> /get_data    Method: GET
        Description: Starts the data scraping process asynchronously. Can run multiple scraping processes.

        Request Headers: None   #TODO  no api key needed!
        Request Body: None

        Response:

            200 OK: Returns a message indicating that the spider is running asynchronously.

    -> /get_proxy   #TODO  bad function unnecessary!

    -> /get_final_data  Method: GET
        Description: Retrieves the final raw data from the whole website. Uses data streaming.

        Request Headers:
            API-Key (string): The API key.

        Request Body: None

        200 OK: Returns the final data as plain text.
        401 Unauthorized: If the API key is invalid
        404 Not Found: If the file is not found.

    -> /get_processed_data  Method: GET
        Description: Retrieves processed data based on the provided API key.
                    The process is defined in the business logic folder.

        Request Headers: 
            API-Key (string): The API key.
        Request Body: None

        Response:
            200 OK: Returns the processed data.
            401 Unauthorized: If no API key is provided or the API key is incorrect.
            500 Internal Server Error: If there is a problem with getting the files from Google Drive.

    -> /customer_data_process Method: GET:
        Description: Runs the data process in the business logic folder.

        Request Headers: 
            API-Key (string): The API key.

        Request Body: None

        200 OK: If the data processing was successful
        401 Unauthorized: If no API key is provided or the API key is incorrect
        500 Internal Server Error: If data processing failed

    -> /get_business_logic_data     Method: GET:
        Description: Retrieves business logic data by name.

        Request Headers:
            shrek_key (string): The client API key.

        Request Body: None

        Query Parameters:
            file_name (string): The name of the file to retrieve. 
            Default is "customer_min_prices.xml".
        
        Response:
            200 OK: Returns the requested file.
            401 Unauthorized: If no API key is provided or the API key is incorrect.
            404 Not Found: If the file does not exist.

    -> /get_top_5_products  Method: GET:
        Description: Api endpoint for getting the top 5 products. (Used for testing)

        Request Headers: None
        Request Body: None

        Response:

            200 OK: Returns the top 5 products as a JSON file.
            404 Not Found: If the file does not exist.
            500 Internal Server Error: If data processing failed.

    -> /start_aprox_scrape  Method: GET:
        #TODO  what dose this realy do?
        Description: Starts the approximate scrape process.

        Request Headers: None
        Request Body: None

        Response:
            200 OK: Returns a message indicating that the spider is running

    -> /get_products_url    Method: GET:
        Description: ??? Do not know what it dose! TODO 

        Request Headers: None !!!! # TODO no api key needed
        Request Body: None

        Response:
            200 OK: Returns the product URLs as a JSON file.
            404 Not Found: If the JSON file is not found
            500 Internal Server Error: If there is an error processing JSON data

    -> /start_url_scrape    Method: POST, GET:
        Description: Starts the URL scrape process with the provided URLs.

        Request Headers: None # TODO  no api key needed !!

        Request Body:
            urls (array): List of URLs to scrape.
            It will be passed as a kwarg to the spider.

        Response:
            200 OK: Returns a message indicating that the spider is running.

    -> /get_url_scrape      Method: GET:
        Description: Retrieves the URL scrape results.

        Request Headers: None   # TODO  no api key needed !!
        Request Body: None

        Response:
            200 OK: Returns the URL scrape results as a JSON file.
            404 Not Found: If the JSON file is not found
            500 Internal Server Error: If there is an error processing JSON data.


    API endpoints defined in api_manager/google_drive/google_drive_api_auth.py:
    !Every google drive authentication data must be provided in the server environment variables.

    #TODO !!there are duplicate api endpoint in the google endpoint!!

    -> /shrek_key_checker METHOD = GET
        Checks if the clients key is correct.

        Possible returns:
            400 - No provided api key
            200 - Correct api key
            403 - Incorrect api key

    -> /list_files METHOD = GET
        Lists the available files in the google drive.

        Possible returns:
            200 :
                {'message': 'No files found'}
                or 
                Json file structured : {name : str, id (google drive file id) : str}
            403 - Incorrect api key
            500 - {'error': 'Authentication failed or an error occurred'}

    -> /get_file/<file_id> METHOD = GET
        Parameter : file_id : str -> Google drive inner file id. Must provide!
        Get the the wanted file id from the /list_files api endpoint.

        When retrieving file even if the file is stored in gzip compression the server returns the uncompressed version.

        Possible returns:
            Directly returns the response from google drive api.
            Uses data streaming. 

            500 : "Error occurred while retrieving the file."

    -> /create_file/<file_name>/<file_mimeType>/<force_update> METHOD = POST
        Uploads a new file in the google drive.

        Parameters:
            - file_name : str - The intended file name - Must provide!
            - file_mimeType : str - The intended file type. Must corresponding to the encoded mime types! - Must provide!
                Possible mime types:
                    - "text" -> plain text format.
                    - "ipynb" -> executable python script.
                    - "csv" -> csv file.
                    - "gzip" -> a compressed gzip file for smaller data amount.
            - force_update : 0/1 - If wanted to forcefully overwrite the file name even if the file is already exists. - Optional.
                - base value = 0 => no overwrite

        Possible returns:
            - 200 {newly created file id.}
            - 400 {'error': 'File name and MIME type are required.'} or {'error': 'File name already exists.'}
            - 403 {'error': 'Invalid API key'}

    -> /delete_file/<file_id> METHOD = GET
        Deletes an uploaded file from google drive.

        Parameters:
            -file_name: str - The name of the file to delete. !Must provide!

        Possible returns:
            -403: { "error": "Invalid API key" }
            -200: "File deleted successfully"
            -404: "File not found"
            -500: "An error occurred"

    -> /run_script METHOD = GET
        Runs a Python script from Google Drive.

        Parameters:
            - shrek_key: str - API key in request headers.
            - file_id: str - The ID of the script file to run in request headers.

        Possible returns:
            -200: Script output.
            -400: "Invalid Python code"
            -403: { "error": "Invalid API key" }
            -500: "An error occurred while running the script."

    -> /upload   METHOD  = POST
    -> /download_uploaded/<file_name>   METHOD = GET

    All the api endpoints rely on data_retrieve.py file.
    It manages the run commands for the different spiders.
    It passes the kwargs for the spiders if needed.

There are three big parts of my application.
Components:
-> Main flask server
-> Scraper
    -> Proxy scraper
-> Google Drive communication
    # gzip coding implemented for shorted data amount.

-------------------------------------------------------------
                   The data flow:

            +---------------------------------+
            |       Flask Application         |
            +---------------------------------+
                            |
                            v
            +---------------------------------+
            |  API Endpoint Triggered         |
            +---------------------------------+
                            |
                            v
            +---------------------------------+
            |    Initiate Scrapy Spider       |
            +---------------------------------+
                            |
                            v
            +---------------------------------+
            |      Scrapy Spider Runs         |
            +---------------------------------+
                            |
                            v
            +---------------------------------+
            |  Save Data Locally (Filesystem) |
            +---------------------------------+
                            |
                            v
            +---------------------------------+
            |  Push Data to Google Drive      |
            +---------------------------------+

------------------------------------------------------------
Main Flask server
------------------------------------------------------------
This is the central server that manages all the components.

main.py


------------------------------------------------------------
Scrapy
------------------------------------------------------------
This part is responsible to run the main scraping part.
It contains multiple spiders for different task.

Location: app\heroku_scrapy\heroku_scrapy\spiders

                        Main scraping methods


        +-----------------------------+------------------------------------------------+
        | Whole Website Scraping                                                       |
        +-----------------------------+------------------------------------------------+
        | Description                 | Scrapes the entire website, capturing all      |
        |                             | accessible pages.                              |
        +-----------------------------+------------------------------------------------+
        | Benefits                    | - Comprehensive data collection                |
        |                             | - Ensures no relevant data is missed           |
        +-----------------------------+------------------------------------------------+
        | Drawbacks                   | - Higher resource consumption                  |
        |                             | - Increased processing time                    |
        +-----------------------------+------------------------------------------------+

        +-----------------------------+------------------------------------------------+
        | External Source URLs                                                         |
        +-----------------------------+------------------------------------------------+
        | Description                 | Fetches URLs from an external source and       |
        |                             | scrapes only the provided URLs.                |
        +-----------------------------+------------------------------------------------+
        | Benefits                    | - Targeted data collection                     |
        |                             | - Lower resource consumption                   |
        +-----------------------------+------------------------------------------------+
        | Drawbacks                   | - Potentially misses relevant data not listed  |
        |                             | - Depends on the accuracy of the provided URLs |
        +-----------------------------+------------------------------------------------+

        The different api calls are responsible for witch method is used.


                          Preferred scraping usage
                        +-------------------------+
                        | Scrape the whole website|
                        |   once a week to capture|
                        |   all new products or   |
                        |   URL changes           |
                        +-------------------------+
                                    |
                                    v
                        +-------------------------+
                        |  Scrape the URLs only   |
                        |        daily            |
                        +-------------------------+
                                                    
#TODO  write the exact api endpoint for this scenario.

All scraping methods implement user agent header rotations and proxy rotation.
User agent headers are stored in a txt file.

Proxies are got from a third party website.
Link : https://free-proxy-list.net
 
Before and during scarping process it refreshes the proxy pool asynchronously.

How dose url_crawl work?

                +---------------------------------+
                |        URL Scrape API           |
                |  /start_url_scrape is triggered |
                |         Endpoint Triggered      |
                +---------------------------------+
                                |
                                v
                +---------------------------------+
                |    run_url_scrape Function      |
                |    Triggered in data_retrieve.py|
                +---------------------------------+
                                |
                                v
                +---------------------------------+
                |         Command is Run          |
                |  I gets the url list from the   |
                |  api endpoint                   |
                +---------------------------------+
                                |
                                v
                +---------------------------------+
                |     URL Scrape Spider Runs      |
                +---------------------------------+



------------------------------------------------------------
Google Drive API
------------------------------------------------------------
    This part is responsible for reading, writing, deleting and creating files to Google Drive.
    Storing data on Google drive is easy and free for a set of data.up to 15 GB.
    ->Thats why i choose to store data in gzip format.

    Google Documentation : https://developers.google.com/drive/api/quickstart/python

    Location : app/api_manager/google_drive

    -> google_drive_api_auth.py
        This part is responsible for authentication for each api call to Google Drive.

        It uses server environment variable.

    -> google_adrive_apiendpoint.py
        Here is defined all the different operation that can be made to Google drive api.



# TODO  delete all chs_get.py files !!